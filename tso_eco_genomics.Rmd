---
title: "Ecological Genomics Notebook"
author: "TS O'Leary"
output:
  rmarkdown::html_document:
    theme: lumen
    number_sections: true
    toc: true
    toc_float: true
---


# Command-line UNIX tutorial

## What is the command-line?

Command-line (_aka_ terminal or shell)
- a way of interacting with your local computer or a remote server 
- commands or scripts, without using a graphical user interface (GUI).

## Why do I want to be doing this?

- It's quite easy to copy, move, edit, and search within thousands of files in multiple directories with some simple command-line code. 
- It would take forever to do this by dragging/dropping with a mouse. 
- Allows you to work with very large data files without uncompressing them fully, or loading the entire file's contents into memory

## Connect to the server

- The first step is to open a terminal *shell* on your local computer. For MacOS users, this is called "Terminal".

- We'll connect to our remote server running Unix using the secure shell (ssh) protocol. Our server's name is *pbio381* and we can connect to it using our UVM netid username and password (as long as we're on-campus)

```bash
ip0af52fbf:papers tsoleary$ ssh tsoleary@pbio381.uvm.edu
tsoleary@pbio381.uvm.edu's password: 
Last login: Tue Jan 31 10:51:15 2017 from ip040027.uvm.edu
[tsoleary@pbio381 ~]$ 
```

- The log-in screen tells us some basic info on when we last logged in, and then gives us our current location in the filesystem (~) followed by the $ prompt, that tells us the computer is ready for our next command. 

  - NOTE: The tilda (~) is short-hand for your home directory in UNIX. This is your own personal little corner of the computer's hard drive space, and is the location that you should use to create folders and input/output/results files that are specific to your own work. No one has access to any files stored in your home directory but you.

- To see the full path to your current directory, use the **pwd** command:

```bash
[tsoleary@pbio381 ~]$ pwd
/users/s/r/tsoleary
[tsoleary@pbio381 ~]$ 
```


- The path shows the full directory address up from the "root" of the file structure, which is the most basal level (appealing to all you phylogeneticists here…). The root is symbolized as "/" and each subdirectory is separated by an additional "/". So, the full path to my working directory on the server is */users/s/r/tsoleary/*


- Let's make a new folder (aka, directory) using the **mkdir** command. Let's name this folder "mydata"

```bash
[tsoleary@pbio381 ~]$ mkdir mydata
```

- We can then use the **ll** command to show the current contents of any folders and files in our current location:

```bash
[tsoleary@pbio381 ~]$ ll
total 0
drwxr-xr-x. 6 tsoleary users 82 Jan 31 17:21 archive
drwxr-xr-x. 2 tsoleary users  6 Jan 31 17:21 mydata
drwxr-xr-x. 2 tsoleary users  6 Jan 31 17:15 scripts
[tsoleary@pbio381 ~]$ 
```

- You'll notive that I've got some extra folders in my output from previous work, whereas you will probably only see the "scripts" folder you just made. 
- NOTE: Each row shows a file or a folder (in this case, these are all folders) diplaying (from right to left) its name, when it was last edited, size, who it belongs to , and who has permission to read (r) write (w) and exectue (x) it. More on permissions later...
- Try making your own folder named "scripts" and then use the `ll` command to list the folders again 
- We can change our current location within the directory structure using the `cd` command. Let's use `cd` to move inside the `mydata/` directory and `ll` to list its contents:

```bash
[tsoleary@pbio381 ~]$ cd mydata/
[tsoleary@pbio381 mydata]$ ll
total 0
[tsoleary@pbio381 mydata]$ 
```

- Hah — nothing in there yet! Let's go get some data!
  - We've placed the text file containing all the metadata information on the seastar sampling under a shared space on the server. The path to this shared space is: 
    - */data/*   Try using **cd** to navigate over to this location. Then **ll** to show its contents. You should see something like this:

```bash
drwxr-xr-x.  5 root root       73 Jan 31 17:35 archive
drwxrwxr-x.  2 root pb381adm   40 Nov 30  2015 packages
drwxrwxr-x. 33 root pb381adm 4096 Nov 30  2015 popgen
drwxrwxr-x.  3 root pb381adm   42 Jan 30 09:08 project_data
drwxrwxr-x.  2 root pb381adm    6 Oct  2  2015 scripts
drwxr-xr-x. 18 root root     4096 Sep  2  2015 users
[tsoleary@pbio381 data]$ 
```

- Now, **cd** into the folder called "project_data" and **ll**. Do you see this?

```bash
[tsoleary@pbio381 data]$ cd project_data/
[tsoleary@pbio381 project_data]$ ll
total 8
drwxr-xr-x. 12 tsoleary users 4096 Jan 30 09:06 archive
-rw-r--r--.  1 tsoleary users 1255 Jan 30 09:08 ssw_samples.txt
[tsoleary@pbio381 project_data]$ 
```

- The file called `ssw_samples.txt` is the one with the seastar metadata. We don't want to open and make changes to this file in the shared space, because we don't want to have our edits affect the rest of the group. So, let's first make a copy of this file over to our home directory and put it inside the "mydata" folder. Use the `cp` command, followed by the filename, and the path to your destination (remember the ~ signals your home directory, and each subdirectory is then separated by a `/`):

```bash
[tsoleary@pbio381 project_data]$ cp ssw_samples.txt ~/mydata/
```

- **cd** back to your *~/mydata/* directory and look inside. You should see your file...

```bash
[tsoleary@pbio381 project_data]$ cd ~/mydata/
[tsoleary@pbio381 mydata]$ ll
total 4
-rw-r--r--. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
[tsoleary@pbio381 mydata]$ 
```

- Let's take a peek at this file with the **head** command, which prints the first 10 lines to screen.

```bash
[tsoleary@pbio381 mydata]$ head ssw_samples.txt 
Individual	Trajectory	Location	Day3	Day6	Day9	Day12	Day15
10	HH	INT	10_5-08_H	10_5-11_H	10_5-14_H	10_5-17_H	10_5-20_H
24	HH	INT	24_5-08_H	24_5-11_H	24_5-14_H	24_5-17_H	24_5-20_H
27	HH	INT	27_5-08_H	27_5-11_H	27_5-14_H	27_5-17_H	27_5-20_H
08	HS	INT	08_5-08_H	08_5-11_S	08_5-14_S	08_5-17_S	08_5-20_S
09	HS	INT	09_5-08_H		09_5-14_S	09_5-17_S	09_5-20_S
15	HS	INT	15_5-08_H	15_5-11_H	15_5-14_H	15_5-17_S	15_5-20_S
19	HS	INT		19_5-11_H	19_5-14_H	19_5-17_H	19_5-20_S
20	HS	INT	20_5-08_H	20_5-11_H	20_5-14_H	20_5-17_H	20_5-20_S
03	SS	INT	03_5-08_S	03_5-11_S
```

- The **tail** command provides similar functionality, but prints just the last lines in the file. These features may not seem a big deal right now, but when you're dealing with files that are 20 Gb compressed, and feature hundreds of millions of lines of data, you and your computer will be happy to have tools to peek inside without having to open the whole file!
- What if we want to extract just the rows of data that correspond to Healthy (HH) individuals? We can use the search tool **grep** to search for a target query. Any line matching our search string will be printed to screen.

```bash
[tsoleary@pbio381 mydata]$ grep 'HH' ssw_samples.txt 
10	HH	INT	10_5-08_H	10_5-11_H	10_5-14_H	10_5-17_H	10_5-20_H
24	HH	INT	24_5-08_H	24_5-11_H	24_5-14_H	24_5-17_H	24_5-20_H
27	HH	INT	27_5-08_H	27_5-11_H	27_5-14_H	27_5-17_H	27_5-20_H
31	HH	SUB	31_6-12_H	31_6-15_H	31_6-18_H	31_6-21_H	31_6-24_H
32	HH	SUB	32_6-12_H	32_6-15_H	32_6-18_H	32_6-21_H	
33	HH	SUB	33_6-12_H	33_6-15_H	33_6-18_H	33_6-21_H	33_6-24_H
34	HH	SUB	34_6-12_H	34_6-15_H	34_6-18_H	34_6-21_H	34_6-24_H
35	HH	SUB	35_6-12_H	35_6-15_H	35_6-18_H	35_6-21_H	
[tsoleary@pbio381 mydata]$
```

- What if instead of printing it to screen, we want to save the output of our search to a new file? This is easy, just use the ">" symbol to redirect the results of any command to an output file with your choice of name.

```bash
[tsoleary@pbio381 mydata]$ grep 'HH' ssw_samples.txt >ssw_HHonly.txt
[tsoleary@pbio381 mydata]$ ll
total 8
-rw-r--r--. 1 tsoleary users  462 Jan 31 20:46 ssw_HHonly.txt
-rwxrwxr-x. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
[tsoleary@pbio381 mydata]$ 
```

- We can do the same routine for the "SS" samples. Here's a trick, when you're doing a similar task as a previous command, hit the up arrow on your keyboard at the $ prompt, and it will recall the last command you issued. Then you just have to switch the HH's for SS's.

```bash
[tsoleary@pbio381 mydata]$ grep 'SS' ssw_samples.txt >ssw_SSonly.txt
[tsoleary@pbio381 mydata]$ ll
total 12
-rw-r--r--. 1 tsoleary users  462 Jan 31 20:46 ssw_HHonly.txt
-rwxrwxr-x. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
-rw-r--r--. 1 tsoleary users  342 Jan 31 20:48 ssw_SSonly.txt
[tsoleary@pbio381 mydata]$ 
```

- `grep` is a useful search tool and has many additional features for sorting and output of the results. These kinds of search algorithms are called "regular expressions", or "regexp", and are one of the most powerful tools for wokring with large text files. If you want to learn more about `grep` and its regexp capabilities, you can look at the `man` page or manual. In fact, every UNIX command-line program has a built-in `man` page that you can call up to help you. Just type `man` and then the program name and it will give you the manual (small excerpt shown below).

```bash
[tsoleary@pbio381 mydata]$ man grep


GREP(1)                            General Commands Manual                           GREP(1)

NAME
       grep, egrep, fgrep - print lines matching a pattern

SYNOPSIS
       grep [OPTIONS] PATTERN [FILE...]
       grep [OPTIONS] [-e PATTERN | -f FILE] [FILE...]

DESCRIPTION
       grep searches the named input FILEs (or standard input if no files are named, or if a
       single hyphen-minus (-) is given as file name) for lines containing a  match  to  the
       given PATTERN.  By default, grep prints the matching lines.

       In  addition,  two variant programs egrep and fgrep are available.  egrep is the same
       as grep -E.  fgrep is the same as grep -F.  Direct  invocation  as  either  egrep  or
       fgrep  is  deprecated,  but is provided to allow historical applications that rely on
       them to run unmodified.

OPTIONS
   Generic Program Information
       --help Print a usage message briefly summarizing these command-line options  and  the
              bug-reporting address, then exit.

       -V, --version
              Print  the version number of grep to the standard output stream.  This version
              number should be included in all bug reports (see below).

   Matcher Selection
       -E, --extended-regexp
              Interpret PATTERN as an extended regular expression (ERE, see below).  (-E  is
              specified by POSIX.)

       -F, --fixed-strings, --fixed-regexp
              Interpret  PATTERN  as  a list of fixed strings, separated by newlines, any of
              which is to be matched.  (-F is  specified  by  POSIX,  --fixed-regexp  is  an
              obsoleted alias, please do not use it in new scripts.)

       -G, --basic-regexp
              Interpret PATTERN as a basic regular expression (BRE, see below).  This is the
              default.

       -P, --perl-regexp
              Interpret PATTERN as a Perl regular expression.  This is  highly  experimental
              and grep -P may warn of unimplemented features.
```

- One of the most useful aspects of UNIX is the ability to take the output from one command and use it as standard input (termed 'stdin') into another command without having to store the intermediate files. Such a workflow is called "piping", and makes use of the pipe character (|) located above the return key to feed data between programs.
  - Example: Say we wanted to know how many samples come from the Intertidal. We can use `grep` to do the search, and pipe the results to the command `wc` which will tally up the number of lines, words, and characters in the file…voila!

```bash
[tsoleary@pbio381 mydata]$ grep 'INT' ssw_samples.txt | wc
     16     106     762
[tsoleary@pbio381 mydata]$ 
```

- Looks like 16 INT samples in the original data. See how quick it was to get a line count on this match, without actully opening a file or printing/saving the outputs? 
- Now, what if we want to move the files we created with just individuals of a particular disease status. There's a way to do this quickly using the wildcard character `*`. With the wildcard, the `*\*` takes the place of any character, and in fact any length of characters.

- Make a new directory called `samples_by_disease/` inside the `mydata/` folder
- Move all files that contain the word "only" into the new directory using the `mv` command.

```bash
[tsoleary@pbio381 mydata]$ mkdir sample_by_disease/
[tsoleary@pbio381 mydata]$ ll
total 12
drwxr-xr-x. 2 tsoleary users   10 Jan 31 21:12 sample_by_disease
-rw-r--r--. 1 tsoleary users  462 Jan 31 20:46 ssw_HHonly.txt
-rwxrwxr-x. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
-rw-r--r--. 1 tsoleary users  342 Jan 31 20:48 ssw_SSonly.txt
[tsoleary@pbio381 mydata]$ mv *only* sample_by_disease/
[tsoleary@pbio381 mydata]$ ll
total 4
drwxr-xr-x. 2 tsoleary users   60 Jan 31 21:12 sample_by_disease
-rwxrwxr-x. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
[tsoleary@pbio381 mydata]$ cd sample_by_disease/
[tsoleary@pbio381 sample_by_disease]$ ll
total 8
-rw-r--r--. 1 tsoleary users 462 Jan 31 20:46 ssw_HHonly.txt
-rw-r--r--. 1 tsoleary users 342 Jan 31 20:48 ssw_SSonly.txt
[tsoleary@pbio381 sample_by_disease]$ 
```

- OK, what about when we have files we don't want anymore? How do we clean up our workspace? You can remove files and folders with the `rm` command. However, in its default mode, UNIX will not ask if you really mean it before getting rid of it forever(!), so this can be dangerous if you're not paying attention. 
- As an example, let's use our `grep` command to pull out he seastar samples that started healthy and then became sick. But perhaps we later decide we're not going to work with those samples, so we use `rm` to delete that file:

```bash
[tsoleary@pbio381 mydata]$ ll
total 8
drwxr-xr-x. 2 tsoleary users   60 Jan 31 21:12 sample_by_disease
-rw-r--r--. 1 tsoleary users  282 Feb  1 05:35 ssw_HSonly.txt
-rwxrwxr-x. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
[tsoleary@pbio381 mydata]$ rm ssw_HSonly.txt 
[tsoleary@pbio381 mydata]$ ll
total 4
drwxr-xr-x. 2 tsoleary users   60 Jan 31 21:12 sample_by_disease
-rwxrwxr-x. 1 tsoleary users 1255 Jan 31 17:42 ssw_samples.txt
[tsoleary@pbio381 mydata]$
```


- Gone! Forever! If that worries you, you can change your personal settings so that the server asks you to confirm deletion before it acts. To do this, we'll need to follow a couple of new steps:

## Confirm deletion after rm

1.    `cd` to your home directory (~/)
2. list all the files, including "hidden" ones that aren't usually shown. To do this, use `ll -a`.
3. Look for a file called ".bashrc" — this contains your settings for how you interact with the server when you log in.
4. We're going to open this file and edit it to add a setting to request that `rm` confirms deletion with us. To edit text files on the fly in UNIX, you can use the built-in text editor, "vim": `vim .bashrc`
5. You should see something that looks like this:

```bash
# .bashrc
# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=
# User specific aliases and functions

```

6.   Use your arrow key to move your cursor down to the last line, below ""# User specific aliases and functions" — this is where we're going to insert our new function.

7.   By defauly, vim is in read-only mode when it opens files. To go into edit mode, press your "i" key (for "insert"). You are now able to make changes to the file.

8.   Add the following text on a new line directly below the "# User specific…" line:

       `alias rm='rm -i'`

9.   Your file should now look like this:

```bash
# .bashrc
# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=
# User specific aliases and functions

alias rm='rm -i'
```

10. You're now ready to get out of edit mode (hit the `escape key`), save your changes (type `:w`), and exit vim (type `:q`).

11. These changes won't take effect until you log out (type `exit` to log out of the server). But from now on, every time you log in, the server will remember that you want a reminder before deleting any of your work.

## Review what we've learned so far

- Logging in to the server: `ssh netid@pbio381.uvm.edu`
- Finding what directory you're in: `pwd`
- Listing files in your current directory, or changing to a new directory: `ll`, `cd`
- Making a new folder: `mkdir foldername`
- Location of shared space, data, and programs on our class server:

```
[tsoleary@pbio381 ~]$ cd /data/
[tsoleary@pbio381 data]$ ll
total 8
drwxr-xr-x.  5 root root       73 Jan 31 17:35 archive
drwxrwxr-x.  2 root pb381adm   40 Nov 30  2015 packages
drwxrwxr-x. 33 root pb381adm 4096 Nov 30  2015 popgen
drwxrwxr-x.  3 root pb381adm   42 Jan 30 09:08 project_data
drwxrwxr-x.  2 root pb381adm    6 Oct  2  2015 scripts
drwxr-xr-x. 18 root root     4096 Sep  2  2015 users
[tsoleary@pbio381 data]$ 
```

- Copying or moving files from one location to another: `cp filename destinationpath/` or `mv filename destinationpath/` 
- Peeking into the first or last few lines of a file: `head filename`, `tail filename`
- Searching within a file for a match: `grep 'search string' filename`
- Outputing the results of a command to a new file: `grep 'search string' filename >outputfilename`
- Using wildcards to work on multiple files at the same time: `mv *.txt ~/newfolder`
-  Using the "pipe" to send the output of one command to the input of another: `grep 'INT' filename | wc `
- Removing files or folders: `rm`
- Editing text files on the server: `vim filename`       

## Useful links

- Handy [UNIX cheat sheet](https://files.fosswire.com/2007/08/fwunixref.pdf) for helping to remember some of these commonly used commands (and others)

- Here's another useful [UNIX cheatsheet](http://cheatsheetworld.com/programming/unix-linux-cheat-sheet/)


# Fastqc, trimming, & mapping

## Learning objectives
- To get background on the ecology of Red spruce (_Picea rubens_), and the experimental design of the exome capture data
- To understand the general work flow or “pipeline” for processing and analyzing the exome capture sequence data
- To visualize and interpret Illumina data quality (what is a fastq file; what are Phred scores?).
- To learn how to make/write a bash script, and how to use bash commands to process files in batches
- To trim the reads based on base quality scores
- To start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome

## Red spruce, _Picea rubens_
 
- Red spruce is a coniferous tree that plays a prominent role in montane communities throughout the Appalachians. 

- Thrives in the cool, moist climates of the high elevation mountains of the Apppalachians and northward along the coastal areas of Atlantic Canada.

One region where populations are particular vulnerable to climate change is in the low-latitude trailing edge of the range, from Maryland to Tennessee, where 
- populations are highly fragmented and isolated on mountaintops. 
- These “island” populations are remnants of spruce forests that covered the southern U.S. glaciers extended as far south as Long Island, NY. 
- As the climate warmed at the end of the Pleistocene (~20K years ago), red spruce retreated upward in elevation to these mountaintop refugia, where they are now highlty isolated from other such stands and from the core of the range further north.


With funding from the National Science Foundation, the Keller Lab is studying the genetic basis of climate adaptation across the distribution of P. rubens. 

Our main goals are to:

1. characterize the genetic diversity and population structure across the range

2. identify regions of the genome that show evidence of positive selection in response to climate gradients

3. map the genetic basis of climate adaptive phenotypes


We hope to use this information to inform areas of the range most likely to experience climate maladaptation, and to help guide mitigation strategies.

## Experimental Design

In 2017, we collected seeds and needle tissue from 340 mother trees at 65 populations spread throughout the range. 
- Extracted whole genomic DNA from needles to use for exome capture sequencing.
    - Sample size in the edge region = 110 mother trees from 23 populations.
- Exome capture was designed based on transcriptomes from multiple tissues and developmental stages in the related species, white spruce (_P. glauca_).
Bait design used 2 transcriptomes previously assembled by Rigault et al. (2011) and Yeaman et al. (2014).
- A total of 80,000 120bp probes were designed, including 75,732 probes within or overlapping exomic regions, and an additional 4,268 probes in intergenic regions.
- Each probe was required to represent a single blast hit to the P. glauca reference genome of at least 90bp long and 85% identity, covering 38,570 unigenes.
- Libraries were made by random mechanical shearing of DNA (250 ng -1ug) to an average size of 400 bp followed by end-repair reaction, ligation of an adenine residue to the 3’-end of the blunt-end fragments to allow the ligation of barcoded adapters, and PCR-amplification of the library. SureSelect probes (Agilent Technologies: Santa Clara, CA) were used for solution-based targeted enrichment of pools of 16 libraries, following the SureSelectxt Target Enrichment System for Illumina Paired-End Multiplexed Sequencing Library protocol.
- Libraries were sequenced on a single run of a Illumina HiSeq X to generate paired-end 150-bp reads.

## The _pipeline_

- Visualize, Clean, Visualize
    - Visualize the quality of raw data (Program: FastQC)
    - Clean raw data (Program: Trimmomatic)
    - Visualize the quality of cleaned data (Program: FastQC)

- Calculate #’s of cleaned, high quality reads going into mapping
- Map (a.k.a. Align) cleaned reads from each sample to the reference assembly to generate sequence alignment files (Program: bwa, Input: .fastq, Output: .sam).
- Remove PCR duplicates identified during mapping, and calculate alignment statistics (% of reads mapping succesully, mapping quality scores, average depth of coverage per individual)
- We’ll then use the results of our mapping next week to start estimating diversity and population structure.
- Visualize, Clean, and Visualize again

Whenever you get a new batch of NGS data, the first step is to look at the data quality of coming off the sequencer and see if we notice any problems with base quality, sequence length, PCR duplicates, or adapter contamination.

### `.fastq` files
A fastq file is the standard sequence data format for NGS. It contains the sequence of the read itself, the corresponding quality scores for each base, and some meta-data about the read.

The files are big (typically many Gb compressed), so we can’t open them completely. Instead, we can peek inside the file using head. But size these files are compressed (note the `.gz` ending in the filenames), and we want them to stay compressed while we peek. Bash has a solution to that called `zcat`. This lets us look at the `.gz` file without uncompressing it all the way.

The fastq files are in this path: `/data/project_data/RS_ExomeSeq/fastq/edge_fastq`

```bash
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq
zcat AB_05_R1_fastq.gz | head -n 4

@GWNJ-0842:368:GW1809211440:2:1101:17168:1907 1:N:0:NGAAGAGA+NTTCGCCT
GATGGGATTAGAGCCCCTGAAGGCTGATAGAACTTGAGTTTCACAGGCTCATTGCATTGAAGTGGCATTTGTGTGAATGCAGAGGAGGTACATAGGTCCTCGAGAATAAAAGAGATGTTGCTCCTCACCAAAATCAGTACAGATTATTTT
+
A<A-F<AFJFJFJA7FJJJJFFJJJJJJ<AJ-FJJ7-A-FJAJJ-JJJA7A7AFJ<FF--<FF7-AJJFJFJA-<A-FAJ<AJJ<JJF--<A-7F-777-FA77---7AJ-JF-FJF-A--AJF-7FJFF77F-A--7<-F--77<JFF<
```

Note: `zcat` lets us open a `.gz` (gzipped) file; we then “pipe” | this output from zcat to the head command and print just the top 4 lines `-n4`.

The `.fastq` file format** has 4 lines for each read:

Line	Description
1. Always begins with ‘@’ and then information about the read
2. The actual DNA sequence
3. Always begins with a ‘+’ and sometimes the same info in line 1
4. A string of characters which represent the quality scores; always has same number of characters as line 2

Here’s a useful reference for understanding Quality (Phred) scores. If P is the probability that a base call is an error, then:

$$P = 10^(–Q/10)$$

$$ Q = –10 log10(P)$$

## Visualize using FastQC
We’re going to use the program FastQC (already installed on our server). FastQC looks at the quality collectively across all reads in a sample.

First, let’s make a new dir within myresults to hold the outputs

```bash
mkdir ~/<myrepo>/myresults/fastqc
```

Then, the basic FastQC command is like this:

```bash
fastqc FILENAME.fastq.gz -o outputdirectory/
```

This will generate an .html output file for each input file you’ve run.

But, we want to be clever and process multiple files (i.e., ALL files from our population) without having to manually submit each one. We can do this by writing a bash script that contains a loop.

The basic syntax of a bash loops is like this:
```bash
for file in myfiles
do
  command 1 -options ${file}
  command 2 -options ${file}
done
```

Note the use of variable assignment using `${}`. We define the word file in the for loop as the variable of interest, and then call the iterations of it using `${file}`. For example, we could use the wildcard character (*) in a loop to call all files that include the population code “AB” and then pass those filenames in a loop to fastqc. Something like:

```bash
for file in AB*fastq.gz

do

 fastqc ${file} -o ~/<myrepo>/myresults/fastqc

done
```

Let’s write the above into a script using the Vim text editor at the command line. Type vim to get into the editor, then type “i” to enter INSERT mode. You can then type your script (remember to make necessary changes to the population code and output path). Lastly, to save the file and quit Vim, hit the ESCAPE key to get out of INSERT mode, followed by `:wq fastqc.sh`

Back at the command line, you should be able to ll and see your new script!

You may find that you need to change the permission on your script to make it executable. Do this using chmod u+x, which changes the permissions to give the user (you) permission to execute (x). Then give it a run!

```bash
chmod u+x fastqc.sh    # makes the script "executable" by the "user"
./fastqc.sh            # executes the script
```

It’ll take just a couple of minutes per fastq file. Once you’ve got results, let’s look at them by pushing your html files up to your Github. Remember,

```bash
git pull
git add --all .
git commit -m "note"
git push
```

Once synced to Github, use Github desktop to pull them down to your laptop where you can open them with a browser.

How does the quality look?

## Clean using Trimmomatic
We’ll use the Trimmomatic program to clean the reads for each file. The program is already installed on our server.

We’ve provided an example script in the `/data/scripts/` directory this time because the program is a java based program and thus a bit more particular in its call.

Copy the bash script over to your `~/myrepo/myscripts` directory
Open and edit the bash script using the program vim.
Edit the file so that you’re trimming the fastq files for the population assigned to you
Change the permissions on your script to make it executable, then run it! (examples below)
```bash
cp /data/scripts/trim_loop.sh  ~/myrepo/myscripts/ # copies the script to your home scripts dir
vim trim_loop.sh    # open the script with vim to edit
```


This time we use the variable coding to call the name of the R1 read pair, define the name for the second read in the pair (R2), and create a basename that only contains the “pop_ind” part of the name, i.e. AB_05
```bash
R2=${R1/_R1_fastq.gz/_R2_fastq.gz}   # defines the name for the second read in the pair (R2) based on knowing the R1 name (the file names are identifcal except for the R1 vs. R2 designation)
f=${R1/_R1_fastq.gz/}   # creates a new variable from R1 that has the "_R1_fastq.gz" stripped off
name=`basename ${f}`   # calls the handy "basename" function to define a new variable containing only the very last part of the filename while stripping off all the path information.  This gets us the "AB_05" bit we want.
```    
Here’s how it should look (replace AB with your population name):

```
#!/bin/bash   
 
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq  

for R1 in AB*R1_fastq.gz  

do 
 
    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}
    f=${R1/_R1_fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
        -threads 1 \
        -phred33 \
         "$R1" \
         "$R2" \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R1.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R1.cl.un.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R2.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R2.cl.un.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        MINLEN:35 
 
done 
```

Trimmomatic performs the cleaning steps in the order they are presented. It’s recommended to clip adapter early in the process and clean for length at the end.

The steps and options are from the Trimmomatic website:

ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
MINLEN: Drop the read if it is below a specified length

## Visualize again using FastQC
- Check the quality of one of your cleaned files using fastqc again
- Redo the same general vizualization steps on the cleaned files

## Mapping against the reference genome

Now that we have cleaned and trimmed read pairs, we’re ready to map them against the reference genome.

### Obtain reference genome

We’ll be using a reduced reference genome based on selecting only those scaffolds of the full genome reference that contain at least one bait. We’ve placed it on our server here:
`/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa`

The reference genome is based on Norway spruce (P. abies) and is available from congenie.org.

### bwa for mapping reads

We’ll use the program bwa, which is a very efficient and very well vetted read mapper. Lots of others exist and can be useful to explore for future datasets. We tried several, and for our exome data, bwa seems to be the best

We are going to write a bash script together that calls the R1 and R2 reads for each individual in our population, and uses the bwa-mem algorithm to map reads to the reference genome. The resulting output will be a `.sam` file alignment. The basic `bwa` command we’ll use is:

```bash
bwa mem -t 1 -M -a ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam

# where:

-t # the number of threads, or computer cpus to use (in this case, just 1)
-M # labels a read with a special flag if its mapping is split across >1 contig
-a # keeps alignments involving unpaired reads
${ref} # specifies the path and filename for the reference genome
${forward} # specifies the path and filename for the cleaned and trimmed R1 reads 
${reverse} # specifies the path and filename for the cleaned and trimmed R2 reads 
>${output}/BWA/${name}.sam  # directs the .sam file to be saved into a directory called BWA
# Other bwa options detailed here: bwa manual page
```

### samtools and sambamba 

Our last steps (with basic syntax) are to: 

1. Convert our `.sam` files to the more efficient binary version .`bam` and sort them `sambamba-0.7.1-linux-static view -S --format=bam file.sam -o file.bam`

2. Get rid of any PCR duplicate sequences (why are these a problem?) and re-sort after removing duplicates.

- `sambamba-0.7.1-linux-static markdup -r -t 1 file.bam file.rmdup.bam`
- `samtools sort file.rmdup.bam -o file.sorted.rmdup.bam`


3. Get some stats on how well the mapping worked
- `samtools flagstat file.sorted.rmdup.bam | awk 'NR>=5&&NR<=13 {print $1}' | column -x`
- `samtools depth file.sorted.rmdup.bam | awk '{sum+=$3} END {print sum/NR}`

For this, we’ll use a combination of two new programs: samtools and sambamba. Samtools was writtend by Heng Li, the same person who wrote bwa, and is a powerful tool for manipulating sam/bam files. Sambamba is derived from samtools, and has been re-coded to increase efficiency (speed). We’ll use them both at different steps.

**I’ve put a bash script with the commands to run steps 1-3 above.** It’s located here:

`/data/scripts/process_bam.sh`

- Make a copy of this over to your home directory 
- Use vim to edit the paths and population
- Save and quit vim, type :wq

### Create a wrapper

Last step, we’re going to put these scripts altogether into a “wrapper” that will exectue each of them one after the other, and work for us while we’re off getting a coffee or sleeping. :) I’ll show you how to code this together in class.

### Using **screen**

Once your wrapper script is ready, you’re going to want to start a **screen**. The screen command initiates a new shell window that won’t interupt or stop your work if you close your computer, log off the server, or leave the UVM network. **Anytime you’re running long jobs, you definiteily want to use screen.**

Using it is easy. Just type screen followed by . It will take you to a new empyt terminal. You can then start your wrapper bash script and see that it starts running. Once everything looks good, you have to detach from the screen by typing `Ctrl-A + Ctrl-D`. If you don’t do this, you’ll lose your work!

When you’re ready to check back on the progress of your program, you can recover your screen by typing `screen -r`. That’ll re-attach you back to your program!


# Mapping the ExomeSeq data

- Review our progress on mapping
- Calculate mapping statistics to assess quality of the result
- Visualize sequence alignment files
- Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences
- Use the ‘ANGSD’ progranm to calculate diversity stats, Fsts, and PCA

## Sequence AlignMent (SAM) files
```bash
/data/project_data/RS_ExomeSeq/mapping/BWA/
```

- First, try looking at a SAM file using head and tail.
```bash
tail -n 100 FILENAME.sam
```

A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, there’s a line in the SAM file that includes

- the read, aka. query, name,
- a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
- the reference sequence name to which the read mapped
- the leftmost position in the reference where the read mapped
- the mapping quality (Phred-scaled)
- a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))
- an ‘=’, mate position, inferred insert size (columns 7,8,9),
- the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
- then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).


The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the ‘@’ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.

Find the official Sequence AlignMent file documentation can be found [here](https://en.wikipedia.org/wiki/SAM_(file_format) or [more officially](https://samtools.github.io/hts-specs/SAMtags.pdf).

- Here’s a [SAM FLAG decoder by the Broad Institute](https://broadinstitute.github.io/picard/explain-flags.html). Use this to decode the second column of numbers

## How well reads mapped to the reference
- We can use the program samtools Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.

- The command `flagstat` gets us some basic info on how well the mapping worked:

- `samtools flagstat FILENAME.sam`

### `bam_stats.sh.`

- We’ll also use the `awk` tool to help format the output.

We’ll use the samtools `flagstat` command to get read counts after mapping, and the depth command to get perhaps the most important statistic in read mapping: the depth of coverage, or how many reads cover each mapped position, on average.

**Put the following into a loop for each individual for which you’ve generated a .sorted.rmdup.bam file:**

- `samtools flagstat file.sorted.rmdup.bam | awk 'NR>=6&&NR<=13 {print $1}' | column -x`
- `>> ${myrepo}/myresults/${mypop}.flagstats.txt`

- `samtools depth file.sorted.rmdup.bam | awk '{sum+=$3} END {print sum/NR}`
- `>> ${myrepo}/myresults/${mypop}.coverage.txt`

While that’s running, we can take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called tview. To use it, simply call the program and command, followed by the sam/bam file you want to view and the path to the reference genome. For example:
```
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/AB_05.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
```

## Calling Genotypes

Inference of population genetics from the aligned sequence data. Should we call genotypes?

Many of the papers you’ll read that do popgen on NGS data have a SNP calling step that results in a specific gneotype being called for each SNP site for each individual. For example,

|SNP|	Ind1|	Ind 2|
|---|-----|------|
|1	|CC|	CT|
|2	|AG|	AA|
|3	|GT|	TT|


But how do we know that Ind1 is homoozygous at SNP-1 (CC) – couldn’t it be CT and we just didn’t have enough coverage to observe the second allele?

The basic problem is that read data are counts that produce a binomial (actually multinomial) distribution of allele calls at a given site, and if you have few reads, you might by chance not observe the true genotype. So, what’s the right thing to do?

As with almost anything in statistics, the right thing to do is not throw away that uncertainty, but instead incorporate it into your analysis. That’s what we’re going to do…

Genotype-free population genetics using genotype likelihoods
A growing movement in popgen analysis of NGS data is embracing the use of genotype likelihoods to calculate stats based on each individual having a likelihood (probability) of being each genotype.

A genotype likelihood (GL) is essentially the probability of observing the sequencing data (reads containing a particular base), given the genotype of the individual at that site.

These probabilities are modeled explicitly in the calculation of population diversty stats like pi, Tajima’s D, Fst, PCA, etc…; thus not throwing out any precious data, but also making fewer assumptions about the true (unknown) genotype at each locus

We’re going to use this approach with the program ‘ANGSD’, which stands for ‘Analysis of Next Generation Sequence Data’

This approach was pioneered by Rasmus Nielsen, published originally in Korneliussen et al. 2014.

ANGSD has a [user’s manual (it’s a work in progress…)](http://www.popgen.dk/angsd/index.php/ANGSD)

The basic work flow of ANGSD looks like this:

Create a list of bam files for the samples you want to analyze
Estimate genotype likelihoods (GL’s) and allele frequencies after filtering to minimize noise
Use GL’s to:
- estimate the site frequency spectrum (SFS)
- estimate nucleotide diversities (Watterson’s theta, pi, Tajima’s D, …)
- estimate Fst between all populations, or pairwise between sets of populations
- perform a genetic PCA based on estimation of the genetic covariance matrix (this is done on the entire set of Edge ind’s)


## ANGSD_mypop.sh

```bash
myrepo="/users/s/r/srkeller/Ecological_Genomics/Spring_2020"

mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="AB"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output}/${mypop}_bam.list
```

Check your output bamlist to see it was made properly.

2. Estimate your GL’s and allele freqs after filtering for depth, base and mapping quality, etc.
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

## Estimating GL's and allele freqs

Estimating GL's and allele frequencies for all sites with ANGSD

```bash
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-doHWE 1 \
# -SNP_pval 1e-6
```

What do all these options mean?

```bash
-nThreads 1	# how many cpus to use – be conservative
-remove_bads 1	# remove reads flagged as ‘bad’ by samtools
-C 50	# enforce downgrading of map quality if contains excessive mismatches
-baq 1	# estimates base alignment qualities for bases around indels
-minMapQ 20	# threshold for minimum read mapping quality (Phred)
-minQ 20	# threshold for minimum base quality (Phred)
-setMinDepth 3	# min read depth across ALL individual to keep a site
-minInd 2	# min number of individuals to keep a site
-setMinDepthInd 1	# min read depth for an individual to keep a site
-setMaxDepthInd 17	# max read depth for an individual to keep a site
-skipTriallelic 1	# don’t use sites with >2 alleles
-GL 1	# estimate GL’s using the Samtools formula
-doCounts 1	# output allele counts for each site
-doMajorMinor 1	# fix major and minor alleles the same across all samples
-doMaf 1 # calculate minor allele frequency
-doSaf 1 # output allele frequencies for each site
-doHWE 1 # calculate obs. v. exp. heterozygosity
-SNP_pval 1e-6 # Keep only site highly likely to be polymorphic (SNPs)
NOTES
```

If you want to restrict the estimation of the genotype likelihoods to a particular set of sites you’re interested in, add the option -sites ./selected_sites.txt (tab delimited file with the position of the site in column 1 and the chromosome in column 2) or use -rf ./selected_chromosome.chrs (if listing just the unique “chromosomes” or contigs you want to anlayze)
Some popgen stats you want to estimate only the polymorphic sites; for this you should include the -SNP_pval 1e-6 option to elininate monomorphic sites when calculating your GL’s
There are good reasons to do it BOTH ways, with and without the -SNP_pval 1e-6 option. Keeping the monomorphic sites in is essential for getting proper estimates of nucleotide diversity and Tajima’s D. But other analyses such as PCA or GWAS want only the SNPs.
3a. Estimate the SFS for your pop

```bash
realSFS {$output}/${mypop}_allsites.saf.idx -maxIter 1000 -tole 1e-6 -P 1 > ${output}/${mypop}_allsites.sfs
```

3b. Once you have the SFS, you can estimate the theta diversity stats:


## Estimating thetas

Estimating thetas for all sites, using the SFS from above as a prior to estimate the GL's

```bash
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop} \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-setMinDepth 3 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-pest ${output}/${mypop}_allsites.sfs \
-doSaf 1 \
-doThetas 1

thetaStat do_stat ${output}/${mypop}_allsites.thetas.idx
```

The first column of the results file (${mypop}.thetas.idx.pestPG) is formatted a bit funny and we don’t really need it. We can use the cut command to get rid of it:

cut -f2- ${mypop}.thetas.idx.pestPG > ${mypop}.thetas
This is now ready to bring into R to look at the mean and variability in nucleotide diversity for our pop. How does it compare to others?

3c. We can calculate Fst between any pair of populations by comparing their SFS to each other. For this, we’ll need to estimate the SFS for pairs of populations; we can each contribute to the overall analysis by looking at how our focal pop is divergent from the others in the edge region
We first want to get a list of the unique population codes that doesn’t include our focal population (it doesn’t make sense to calculate an Fst of a population with itself…)
cat /data/project_data/RS_ExomeSeq/metadata/RS_Exome_metadata.txt | grep -w "E" | cut -f1 | uniq | grep -v ${mypop} > ~/yourrepo/mydata/edgepops_no${mypop}.txt


Next set up a loop to calculate Fst your population and each other pop (coded in the loop as variable “POP2”:

```
for POP2 in `cat ${myrepo}/mydata/edgepops_no${mypop}.txt`
do
    realSFS ${output}/${mypop}_allsites.saf.idx ${output}/${POP2}_allsites.saf.idx -P 1 > ${output}/${mypop}_${POP2}_allsites.sfs
    realSFS fst index ${output}/${mypop}_allsites.saf.idx ${output}/${POP2}_allsites.saf.idx -sfs ${output}/${mypop}_${POP2}_allsites.sfs -fstout ${output}/${mypop}_${POP2}_allsites -whichFst 1
    realSFS fst print ${output}/${mypop}_${POP2}_allsites.fst.idx > ${output}/${mypop}_${POP2}_allsites.fst
done
```

## Ancestory relationships

3d. If we have time (and the previous analysis I started yesterday finally finishes running!), we can look at the ancestry relatioships among all the edge individuals using a genetic Principal Component Analysis. ANGSD has a routine for that too – PCAngsd. First, we estimate the GL’s for the entire sample, keeping just the polymorphic sites, and outputing the GL’s in “Beagle” format (file.beagle.gz)

Make a list of all the sorted and PCR dup removed bam files:

ls /data/project_data/RS_ExomeSeq/mapping/BWA/*sorted.rm*bam >${myrepo}/myresults/EDGE_bam.list

## Run ANGSD to estimate the GL's

```bash
ANGSD -b ${myrepo}/myresults/EDGE_bam.list \
-ref ${REF} -anc ${REF} -out ${myrepo}/myresults/EDGE_poly \
-nThreads 10 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doGlf 2 \
-SNP_pval 1e-6
```

The resulting GL file EDGE_poly.beagle.gz can be used as input to PCAngsd to estimate the covariance matrix:

## PCA - Estimating the covariance matrix with pcangsd
```bash
python /data/popgen/pcangsd/pcangsd.py -beagle ${myrepo}/myresults/EDGE_poly.beagle.gz -o ${myrepo}/myresults/EDGE_poly_covmatrix -threads 1
```

Once you have that, you’ll want to send all your outputs up to Github and then back down to your local machine. We can use R to find the eigenvectors of the genetic covariance matrix and plot it:

## Vizualizing in R

```r
setwd("path to your repo")
covmat <- read.table("myresults/myresults/EDGE_poly_covmatrix")

PCA <- eigen(covmat)
plot(PCA[,1],PCA[,2])
```

